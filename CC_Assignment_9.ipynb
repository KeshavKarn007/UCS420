{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxXZoUPP4zTz+HYFHEKH4J"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports,\n",
        "technology, food, books, etc.).\n",
        "1. Convert text to lowercase and remove punctuaƟon.\n",
        "2. Tokenize the text into words and sentences.\n",
        "3. Remove stopwords (using NLTK's stopwords list).\n",
        "4. Display word frequency distribuƟon (excluding stopwords)."
      ],
      "metadata": {
        "id": "8MIa_x_tEkhJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoNRDyy2ACAf",
        "outputId": "019d3010-1387-45fa-dc75-3a738228fad5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Frequency Distribution (without stopwords):\n",
            "technology: 2\n",
            "evolving: 1\n",
            "rapidly: 1\n",
            "todays: 1\n",
            "world: 1\n",
            "artificial: 1\n",
            "intelligence: 1\n",
            "machine: 1\n",
            "learning: 1\n",
            "robotics: 1\n",
            "transforming: 1\n",
            "industries: 1\n",
            "smartphones: 1\n",
            "smart: 1\n",
            "homes: 1\n",
            "everything: 1\n",
            "becoming: 1\n",
            "connected: 1\n",
            "efficient: 1\n",
            "also: 1\n",
            "improved: 1\n",
            "communication: 1\n",
            "healthcare: 1\n",
            "education: 1\n",
            "future: 1\n",
            "likely: 1\n",
            "bring: 1\n",
            "innovations: 1\n",
            "cant: 1\n",
            "even: 1\n",
            "imagine: 1\n",
            "yet: 1\n",
            "enjoy: 1\n",
            "exploring: 1\n",
            "latest: 1\n",
            "trends: 1\n",
            "gadgets: 1\n",
            "tech: 1\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab') # Download the punkt_tab data\n",
        "\n",
        "# -------- Q1 --------\n",
        "\n",
        "# Favorite topic paragraph\n",
        "text = \"\"\"Technology is evolving rapidly in today's world. Artificial intelligence, machine learning, and robotics are transforming industries.\n",
        "From smartphones to smart homes, everything is becoming connected and efficient. Technology has also improved communication, healthcare, and education.\n",
        "The future will likely bring innovations that we can't even imagine yet. I enjoy exploring the latest trends and gadgets in tech.\"\"\"\n",
        "\n",
        "# 1. Convert to lowercase and remove punctuation\n",
        "text_lower = text.lower()\n",
        "text_no_punct = text_lower.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# 2. Tokenize into words and sentences\n",
        "words = word_tokenize(text_no_punct)\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "# 3. Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "# 4. Word frequency distribution\n",
        "freq_dist = nltk.FreqDist(filtered_words)\n",
        "\n",
        "print(\"Word Frequency Distribution (without stopwords):\")\n",
        "for word, freq in freq_dist.items():\n",
        "    print(f\"{word}: {freq}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: Stemming and LemmaƟzaƟon\n",
        "1. Take the tokenized words from QuesƟon 1 (aŌer stopword removal).\n",
        "2. Apply stemming using NLTK's PorterStemmer and LancasterStemmer.\n",
        "3. Apply lemmaƟzaƟon using NLTK's WordNetLemmaƟzer.\n",
        "4. Compare and display results of both techniques."
      ],
      "metadata": {
        "id": "f-irX8x3FFqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Use filtered_words from above\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# 2. Stemming\n",
        "porter_stemmed = [porter.stem(word) for word in filtered_words]\n",
        "lancaster_stemmed = [lancaster.stem(word) for word in filtered_words]\n",
        "\n",
        "# 3. Lemmatization\n",
        "lemmatized = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "\n",
        "# 4. Comparison\n",
        "print(\"\\nComparison of Stemming and Lemmatization:\")\n",
        "print(f\"{'Original':<15}{'Porter':<15}{'Lancaster':<15}{'Lemma'}\")\n",
        "for i in range(len(filtered_words)):\n",
        "    print(f\"{filtered_words[i]:<15}{porter_stemmed[i]:<15}{lancaster_stemmed[i]:<15}{lemmatized[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMcYDPjRFIrJ",
        "outputId": "8885ecdd-177a-4f58-d554-7167f7ccc154"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Comparison of Stemming and Lemmatization:\n",
            "Original       Porter         Lancaster      Lemma\n",
            "technology     technolog      technolog      technology\n",
            "evolving       evolv          evolv          evolving\n",
            "rapidly        rapidli        rapid          rapidly\n",
            "todays         today          today          today\n",
            "world          world          world          world\n",
            "artificial     artifici       art            artificial\n",
            "intelligence   intellig       intellig       intelligence\n",
            "machine        machin         machin         machine\n",
            "learning       learn          learn          learning\n",
            "robotics       robot          robot          robotics\n",
            "transforming   transform      transform      transforming\n",
            "industries     industri       industry       industry\n",
            "smartphones    smartphon      smartphon      smartphones\n",
            "smart          smart          smart          smart\n",
            "homes          home           hom            home\n",
            "everything     everyth        everyth        everything\n",
            "becoming       becom          becom          becoming\n",
            "connected      connect        connect        connected\n",
            "efficient      effici         efficy         efficient\n",
            "technology     technolog      technolog      technology\n",
            "also           also           also           also\n",
            "improved       improv         improv         improved\n",
            "communication  commun         commun         communication\n",
            "healthcare     healthcar      healthc        healthcare\n",
            "education      educ           educ           education\n",
            "future         futur          fut            future\n",
            "likely         like           lik            likely\n",
            "bring          bring          bring          bring\n",
            "innovations    innov          innov          innovation\n",
            "cant           cant           cant           cant\n",
            "even           even           ev             even\n",
            "imagine        imagin         imagin         imagine\n",
            "yet            yet            yet            yet\n",
            "enjoy          enjoy          enjoy          enjoy\n",
            "exploring      explor         expl           exploring\n",
            "latest         latest         latest         latest\n",
            "trends         trend          trend          trend\n",
            "gadgets        gadget         gadget         gadget\n",
            "tech           tech           tech           tech\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Regular Expressions and Text Spliƫng\n",
        "1. Take their original text from QuesƟon 1.\n",
        "2. Use regular expressions to:\n",
        "a. Extract all words with more than 5 leƩers.\n",
        "b. Extract all numbers (if any exist in their text).\n",
        "c. Extract all capitalized words.\n",
        "3. Use text spliƫng techniques to:\n",
        "a. Split the text into words containing only alphabets (removing digits and special\n",
        "characters).\n",
        "b. Extract words starƟng with a vowel."
      ],
      "metadata": {
        "id": "kwDX2PnfFOXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use original text\n",
        "print(\"\\n--- Regular Expressions ---\")\n",
        "# a. Words with more than 5 letters\n",
        "long_words = re.findall(r'\\b\\w{6,}\\b', text)\n",
        "print(\"Words with >5 letters:\", long_words)\n",
        "\n",
        "# b. Extract numbers\n",
        "numbers = re.findall(r'\\b\\d+\\b', text)\n",
        "print(\"Numbers:\", numbers)\n",
        "\n",
        "# c. Capitalized words\n",
        "capitalized_words = re.findall(r'\\b[A-Z][a-z]*\\b', text)\n",
        "print(\"Capitalized Words:\", capitalized_words)\n",
        "\n",
        "# Text Splitting\n",
        "# a. Words containing only alphabets\n",
        "alpha_words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
        "print(\"Alphabet-only Words:\", alpha_words)\n",
        "\n",
        "# b. Words starting with vowels\n",
        "vowel_words = [word for word in alpha_words if word.lower().startswith(('a','e','i','o','u'))]\n",
        "print(\"Words Starting with Vowels:\", vowel_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqsU3NdDFTIj",
        "outputId": "e8f3f248-09c4-47db-be77-986b19cb5948"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Regular Expressions ---\n",
            "Words with >5 letters: ['Technology', 'evolving', 'rapidly', 'Artificial', 'intelligence', 'machine', 'learning', 'robotics', 'transforming', 'industries', 'smartphones', 'everything', 'becoming', 'connected', 'efficient', 'Technology', 'improved', 'communication', 'healthcare', 'education', 'future', 'likely', 'innovations', 'imagine', 'exploring', 'latest', 'trends', 'gadgets']\n",
            "Numbers: []\n",
            "Capitalized Words: ['Technology', 'Artificial', 'From', 'Technology', 'The', 'I']\n",
            "Alphabet-only Words: ['Technology', 'is', 'evolving', 'rapidly', 'in', 'today', 's', 'world', 'Artificial', 'intelligence', 'machine', 'learning', 'and', 'robotics', 'are', 'transforming', 'industries', 'From', 'smartphones', 'to', 'smart', 'homes', 'everything', 'is', 'becoming', 'connected', 'and', 'efficient', 'Technology', 'has', 'also', 'improved', 'communication', 'healthcare', 'and', 'education', 'The', 'future', 'will', 'likely', 'bring', 'innovations', 'that', 'we', 'can', 't', 'even', 'imagine', 'yet', 'I', 'enjoy', 'exploring', 'the', 'latest', 'trends', 'and', 'gadgets', 'in', 'tech']\n",
            "Words Starting with Vowels: ['is', 'evolving', 'in', 'Artificial', 'intelligence', 'and', 'are', 'industries', 'everything', 'is', 'and', 'efficient', 'also', 'improved', 'and', 'education', 'innovations', 'even', 'imagine', 'I', 'enjoy', 'exploring', 'and', 'in']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Custom TokenizaƟon & Regex-based Text Cleaning\n",
        "1. Take original text from QuesƟon 1.\n",
        "2. Write a custom tokenizaƟon funcƟon that:\n",
        "a. Removes punctuaƟon and special symbols, but keeps contracƟons (e.g.,\n",
        "\"isn't\" should not be split into \"is\" and \"n't\").\n",
        "b. Handles hyphenated words as a single token (e.g., \"state-of-the-art\" remains\n",
        "a single token).\n",
        "c. Tokenizes numbers separately but keeps decimal numbers intact (e.g., \"3.14\"\n",
        "should remain as is).\n",
        "\n",
        "3. Use Regex SubsƟtuƟons (re.sub) to:\n",
        "a. Replace email addresses with '<EMAIL>' placeholder.\n",
        "b. Replace URLs with '<URL>' placeholder.\n",
        "c. Replace phone numbers (formats: 123-456-7890 or +91 9876543210) with\n",
        "'<PHONE>' placeholder."
      ],
      "metadata": {
        "id": "ds95phZ_FZiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Tokenizer\n",
        "def custom_tokenizer(text):\n",
        "    # Keep contractions, handle hyphenated words, keep decimals\n",
        "    text = re.sub(r'[^\\w\\s\\.\\-\\'’]', '', text)  # remove punctuation except . - '\n",
        "    tokens = re.findall(r\"\\b(?:\\d+\\.\\d+|\\d+|[a-zA-Z]+(?:['’][a-zA-Z]+)?(?:-[a-zA-Z]+)*)\\b\", text)\n",
        "    return tokens\n",
        "\n",
        "tokens_custom = custom_tokenizer(text)\n",
        "print(\"\\nCustom Tokens:\", tokens_custom)\n",
        "\n",
        "# Regex Substitutions\n",
        "sample_text = \"Contact me at test.email@example.com or visit https://example.com. Call me at 123-456-7890 or +91 9876543210.\"\n",
        "\n",
        "# a. Replace email addresses\n",
        "sample_text = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', 'keshav@gmail.com', sample_text)\n",
        "\n",
        "# b. Replace URLs\n",
        "sample_text = re.sub(r'https?://\\S+|www\\.\\S+', 'www.keshav.com', sample_text)\n",
        "\n",
        "# c. Replace phone numbers\n",
        "sample_text = re.sub(r'(\\+91\\s?\\d{10}|\\d{3}-\\d{3}-\\d{4})', '+91-98650000', sample_text)\n",
        "\n",
        "print(\"\\nAfter Regex Substitutions:\", sample_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgvhIxpIFcos",
        "outputId": "6b2c8d56-96de-4a6e-a8d4-2ae99255d624"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Custom Tokens: ['Technology', 'is', 'evolving', 'rapidly', 'in', \"today's\", 'world', 'Artificial', 'intelligence', 'machine', 'learning', 'and', 'robotics', 'are', 'transforming', 'industries', 'From', 'smartphones', 'to', 'smart', 'homes', 'everything', 'is', 'becoming', 'connected', 'and', 'efficient', 'Technology', 'has', 'also', 'improved', 'communication', 'healthcare', 'and', 'education', 'The', 'future', 'will', 'likely', 'bring', 'innovations', 'that', 'we', \"can't\", 'even', 'imagine', 'yet', 'I', 'enjoy', 'exploring', 'the', 'latest', 'trends', 'and', 'gadgets', 'in', 'tech']\n",
            "\n",
            "After Regex Substitutions: Contact me at keshav@gmail.com or visit www.keshav.com Call me at +91-98650000 or +91-98650000.\n"
          ]
        }
      ]
    }
  ]
}